{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So,in this tutorial we will implement Reforcement learning Agent ,see how reforcement learning works,and train our agent for game carpole_model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reinforcement learning (RL) is concerned with how agents had to take actions in an environment in order to maximize reward.\n",
    "- It is a hit and trail method\n",
    "- It learns through feedback \n",
    "- There is a concept of Agent,Environment and goal.\n",
    "- Goal is to maximize the reward. \n",
    "- So, at each timestamp you can take one action from Action set A and execute on the enviroment that will change the state of the environment and then environment gives you the reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementation you should know the concept of Q-learning,exploration,explotation \n",
    "Q-learning:\n",
    "    - This algo of reforcement learning used to find best action in a given state so to maximize the reward\n",
    "    - When Q-learning is applied then q-table is formed of shape [state,action]\n",
    "    - This value is stored after an episodes\n",
    "Agent interact with the environment in 2 ways: exploration and explotation\n",
    "    1. Exploration: \n",
    "        - we select an action randomly\n",
    "        - Good at start\n",
    "        - It helps to discover new states\n",
    "    2. explotation:\n",
    "        - we will select the action based on past experience\n",
    "        - Good at end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGENT DESIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self,state_size,action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95 #Discount Factor\n",
    "        self.epsilon = 1.0 # Exploration Rate: How much to act randomly, \n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.learning_rate = 0.001 \n",
    "        self.model = self._create_model()\n",
    "        \n",
    "    \n",
    "    def _create_model(self):\n",
    "        #Neural Network To Approximate Q-Value function\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24,input_dim=self.state_size,activation='relu')) #1st Hidden Layer\n",
    "        model.add(Dense(24,activation='relu')) #2nd Hidden Layer\n",
    "        model.add(Dense(self.action_size,activation='linear'))\n",
    "        model.compile(loss='mse',optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def remember(self,state,action,reward,next_state,done):\n",
    "        self.memory.append((state,action,reward,next_state,done)) #remembering previous experiences\n",
    "        \n",
    "    def act(self,state):\n",
    "        # Exploration vs Exploitation\n",
    "        if np.random.rand()<=self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state) # predict reward value based upon current state\n",
    "        return np.argmax(act_values[0]) #Left or Right\n",
    "    \n",
    "    def train(self,batch_size=32): #method that trains NN with experiences sampled from memory\n",
    "        minibatch = random.sample(self.memory,batch_size)\n",
    "        for state,action,reward,next_state,done in minibatch:\n",
    "            \n",
    "            if not done: #boolean \n",
    "                target = reward + self.gamma*np.amax(self.model.predict(next_state)[0])\n",
    "            else:\n",
    "                target = reward\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state,target_f,epochs=1,verbose=0) #single epoch, x =state, y = target_f, loss--> target_f - \n",
    "            \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def load(self,name):\n",
    "        self.model.load_weights(name)\n",
    "    def save(self,name):\n",
    "        self.model.save_weights(name)\n",
    "            \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes=1000\n",
    "output_dir='C:/Users/J P PANDEY/Desktop/codingBlocks/Reinforcement learning/carpole_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=4,action_size=2)\n",
    "done = False\n",
    "state_size = 4\n",
    "action_size =2\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode :0/1000, High Score:16,Exploration Rate:1.0\n",
      "Game Episode :1/1000, High Score:13,Exploration Rate:1.0\n",
      "Game Episode :2/1000, High Score:78,Exploration Rate:1.0\n",
      "Game Episode :3/1000, High Score:13,Exploration Rate:0.99\n",
      "Game Episode :4/1000, High Score:10,Exploration Rate:0.99\n",
      "Game Episode :5/1000, High Score:37,Exploration Rate:0.99\n",
      "Game Episode :6/1000, High Score:28,Exploration Rate:0.98\n",
      "Game Episode :7/1000, High Score:24,Exploration Rate:0.98\n",
      "Game Episode :8/1000, High Score:12,Exploration Rate:0.97\n",
      "Game Episode :9/1000, High Score:19,Exploration Rate:0.97\n",
      "Game Episode :10/1000, High Score:17,Exploration Rate:0.96\n",
      "Game Episode :11/1000, High Score:21,Exploration Rate:0.96\n",
      "Game Episode :12/1000, High Score:14,Exploration Rate:0.95\n",
      "Game Episode :13/1000, High Score:16,Exploration Rate:0.95\n",
      "Game Episode :14/1000, High Score:32,Exploration Rate:0.94\n",
      "Game Episode :15/1000, High Score:15,Exploration Rate:0.94\n",
      "Game Episode :16/1000, High Score:23,Exploration Rate:0.93\n",
      "Game Episode :17/1000, High Score:11,Exploration Rate:0.93\n",
      "Game Episode :18/1000, High Score:15,Exploration Rate:0.92\n",
      "Game Episode :19/1000, High Score:12,Exploration Rate:0.92\n",
      "Game Episode :20/1000, High Score:19,Exploration Rate:0.91\n",
      "Game Episode :21/1000, High Score:19,Exploration Rate:0.91\n",
      "Game Episode :22/1000, High Score:30,Exploration Rate:0.9\n",
      "Game Episode :23/1000, High Score:11,Exploration Rate:0.9\n",
      "Game Episode :24/1000, High Score:10,Exploration Rate:0.9\n",
      "Game Episode :25/1000, High Score:34,Exploration Rate:0.89\n",
      "Game Episode :26/1000, High Score:9,Exploration Rate:0.89\n",
      "Game Episode :27/1000, High Score:17,Exploration Rate:0.88\n",
      "Game Episode :28/1000, High Score:52,Exploration Rate:0.88\n",
      "Game Episode :29/1000, High Score:17,Exploration Rate:0.87\n",
      "Game Episode :30/1000, High Score:27,Exploration Rate:0.87\n",
      "Game Episode :31/1000, High Score:19,Exploration Rate:0.86\n",
      "Game Episode :32/1000, High Score:28,Exploration Rate:0.86\n",
      "Game Episode :33/1000, High Score:15,Exploration Rate:0.86\n",
      "Game Episode :34/1000, High Score:12,Exploration Rate:0.85\n",
      "Game Episode :35/1000, High Score:9,Exploration Rate:0.85\n",
      "Game Episode :36/1000, High Score:14,Exploration Rate:0.84\n",
      "Game Episode :37/1000, High Score:18,Exploration Rate:0.84\n",
      "Game Episode :38/1000, High Score:11,Exploration Rate:0.83\n",
      "Game Episode :39/1000, High Score:17,Exploration Rate:0.83\n",
      "Game Episode :40/1000, High Score:14,Exploration Rate:0.83\n",
      "Game Episode :41/1000, High Score:10,Exploration Rate:0.82\n",
      "Game Episode :42/1000, High Score:31,Exploration Rate:0.82\n",
      "Game Episode :43/1000, High Score:10,Exploration Rate:0.81\n",
      "Game Episode :44/1000, High Score:49,Exploration Rate:0.81\n",
      "Game Episode :45/1000, High Score:26,Exploration Rate:0.81\n",
      "Game Episode :46/1000, High Score:48,Exploration Rate:0.8\n",
      "Game Episode :47/1000, High Score:21,Exploration Rate:0.8\n",
      "Game Episode :48/1000, High Score:14,Exploration Rate:0.79\n",
      "Game Episode :49/1000, High Score:10,Exploration Rate:0.79\n",
      "Game Episode :50/1000, High Score:23,Exploration Rate:0.79\n",
      "Game Episode :51/1000, High Score:16,Exploration Rate:0.78\n",
      "Game Episode :52/1000, High Score:8,Exploration Rate:0.78\n",
      "Game Episode :53/1000, High Score:12,Exploration Rate:0.77\n",
      "Game Episode :54/1000, High Score:7,Exploration Rate:0.77\n",
      "Game Episode :55/1000, High Score:23,Exploration Rate:0.77\n",
      "Game Episode :56/1000, High Score:12,Exploration Rate:0.76\n",
      "Game Episode :57/1000, High Score:19,Exploration Rate:0.76\n",
      "Game Episode :58/1000, High Score:13,Exploration Rate:0.76\n",
      "Game Episode :59/1000, High Score:9,Exploration Rate:0.75\n",
      "Game Episode :60/1000, High Score:21,Exploration Rate:0.75\n",
      "Game Episode :61/1000, High Score:21,Exploration Rate:0.74\n",
      "Game Episode :62/1000, High Score:11,Exploration Rate:0.74\n",
      "Game Episode :63/1000, High Score:11,Exploration Rate:0.74\n",
      "Game Episode :64/1000, High Score:9,Exploration Rate:0.73\n",
      "Game Episode :65/1000, High Score:11,Exploration Rate:0.73\n",
      "Game Episode :66/1000, High Score:19,Exploration Rate:0.73\n",
      "Game Episode :67/1000, High Score:7,Exploration Rate:0.72\n",
      "Game Episode :68/1000, High Score:44,Exploration Rate:0.72\n",
      "Game Episode :69/1000, High Score:15,Exploration Rate:0.71\n",
      "Game Episode :70/1000, High Score:16,Exploration Rate:0.71\n",
      "Game Episode :71/1000, High Score:17,Exploration Rate:0.71\n",
      "Game Episode :72/1000, High Score:11,Exploration Rate:0.7\n",
      "Game Episode :73/1000, High Score:20,Exploration Rate:0.7\n",
      "Game Episode :74/1000, High Score:30,Exploration Rate:0.7\n",
      "Game Episode :75/1000, High Score:18,Exploration Rate:0.69\n",
      "Game Episode :76/1000, High Score:19,Exploration Rate:0.69\n",
      "Game Episode :77/1000, High Score:30,Exploration Rate:0.69\n",
      "Game Episode :78/1000, High Score:24,Exploration Rate:0.68\n",
      "Game Episode :79/1000, High Score:33,Exploration Rate:0.68\n",
      "Game Episode :80/1000, High Score:11,Exploration Rate:0.68\n",
      "Game Episode :81/1000, High Score:15,Exploration Rate:0.67\n",
      "Game Episode :82/1000, High Score:12,Exploration Rate:0.67\n",
      "Game Episode :83/1000, High Score:17,Exploration Rate:0.67\n",
      "Game Episode :84/1000, High Score:27,Exploration Rate:0.66\n",
      "Game Episode :85/1000, High Score:16,Exploration Rate:0.66\n",
      "Game Episode :86/1000, High Score:33,Exploration Rate:0.66\n",
      "Game Episode :87/1000, High Score:12,Exploration Rate:0.65\n",
      "Game Episode :88/1000, High Score:12,Exploration Rate:0.65\n",
      "Game Episode :89/1000, High Score:18,Exploration Rate:0.65\n",
      "Game Episode :90/1000, High Score:12,Exploration Rate:0.64\n",
      "Game Episode :91/1000, High Score:39,Exploration Rate:0.64\n",
      "Game Episode :92/1000, High Score:37,Exploration Rate:0.64\n",
      "Game Episode :93/1000, High Score:32,Exploration Rate:0.63\n",
      "Game Episode :94/1000, High Score:11,Exploration Rate:0.63\n",
      "Game Episode :95/1000, High Score:36,Exploration Rate:0.63\n",
      "Game Episode :96/1000, High Score:15,Exploration Rate:0.62\n",
      "Game Episode :97/1000, High Score:24,Exploration Rate:0.62\n",
      "Game Episode :98/1000, High Score:21,Exploration Rate:0.62\n",
      "Game Episode :99/1000, High Score:21,Exploration Rate:0.61\n",
      "Game Episode :100/1000, High Score:40,Exploration Rate:0.61\n",
      "Game Episode :101/1000, High Score:29,Exploration Rate:0.61\n",
      "Game Episode :102/1000, High Score:65,Exploration Rate:0.61\n",
      "Game Episode :103/1000, High Score:38,Exploration Rate:0.6\n",
      "Game Episode :104/1000, High Score:34,Exploration Rate:0.6\n",
      "Game Episode :105/1000, High Score:26,Exploration Rate:0.6\n",
      "Game Episode :106/1000, High Score:46,Exploration Rate:0.59\n",
      "Game Episode :107/1000, High Score:31,Exploration Rate:0.59\n",
      "Game Episode :108/1000, High Score:30,Exploration Rate:0.59\n",
      "Game Episode :109/1000, High Score:46,Exploration Rate:0.58\n",
      "Game Episode :110/1000, High Score:20,Exploration Rate:0.58\n",
      "Game Episode :111/1000, High Score:26,Exploration Rate:0.58\n",
      "Game Episode :112/1000, High Score:28,Exploration Rate:0.58\n",
      "Game Episode :113/1000, High Score:62,Exploration Rate:0.57\n",
      "Game Episode :114/1000, High Score:29,Exploration Rate:0.57\n",
      "Game Episode :115/1000, High Score:23,Exploration Rate:0.57\n",
      "Game Episode :116/1000, High Score:21,Exploration Rate:0.56\n",
      "Game Episode :117/1000, High Score:22,Exploration Rate:0.56\n",
      "Game Episode :118/1000, High Score:19,Exploration Rate:0.56\n",
      "Game Episode :119/1000, High Score:37,Exploration Rate:0.56\n",
      "Game Episode :120/1000, High Score:38,Exploration Rate:0.55\n",
      "Game Episode :121/1000, High Score:9,Exploration Rate:0.55\n",
      "Game Episode :122/1000, High Score:27,Exploration Rate:0.55\n",
      "Game Episode :123/1000, High Score:18,Exploration Rate:0.55\n",
      "Game Episode :124/1000, High Score:29,Exploration Rate:0.54\n",
      "Game Episode :125/1000, High Score:9,Exploration Rate:0.54\n",
      "Game Episode :126/1000, High Score:24,Exploration Rate:0.54\n",
      "Game Episode :127/1000, High Score:57,Exploration Rate:0.53\n",
      "Game Episode :128/1000, High Score:17,Exploration Rate:0.53\n",
      "Game Episode :129/1000, High Score:30,Exploration Rate:0.53\n",
      "Game Episode :130/1000, High Score:34,Exploration Rate:0.53\n",
      "Game Episode :131/1000, High Score:58,Exploration Rate:0.52\n",
      "Game Episode :132/1000, High Score:49,Exploration Rate:0.52\n",
      "Game Episode :133/1000, High Score:24,Exploration Rate:0.52\n",
      "Game Episode :134/1000, High Score:24,Exploration Rate:0.52\n",
      "Game Episode :135/1000, High Score:63,Exploration Rate:0.51\n",
      "Game Episode :136/1000, High Score:21,Exploration Rate:0.51\n",
      "Game Episode :137/1000, High Score:14,Exploration Rate:0.51\n",
      "Game Episode :138/1000, High Score:24,Exploration Rate:0.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Episode :139/1000, High Score:24,Exploration Rate:0.5\n",
      "Game Episode :140/1000, High Score:21,Exploration Rate:0.5\n",
      "Game Episode :141/1000, High Score:27,Exploration Rate:0.5\n",
      "Game Episode :142/1000, High Score:25,Exploration Rate:0.5\n",
      "Game Episode :143/1000, High Score:111,Exploration Rate:0.49\n",
      "Game Episode :144/1000, High Score:18,Exploration Rate:0.49\n",
      "Game Episode :145/1000, High Score:30,Exploration Rate:0.49\n",
      "Game Episode :146/1000, High Score:33,Exploration Rate:0.49\n",
      "Game Episode :147/1000, High Score:19,Exploration Rate:0.48\n",
      "Game Episode :148/1000, High Score:12,Exploration Rate:0.48\n",
      "Game Episode :149/1000, High Score:37,Exploration Rate:0.48\n",
      "Game Episode :150/1000, High Score:32,Exploration Rate:0.48\n",
      "Game Episode :151/1000, High Score:63,Exploration Rate:0.47\n",
      "Game Episode :152/1000, High Score:30,Exploration Rate:0.47\n",
      "Game Episode :153/1000, High Score:35,Exploration Rate:0.47\n",
      "Game Episode :154/1000, High Score:19,Exploration Rate:0.47\n",
      "Game Episode :155/1000, High Score:19,Exploration Rate:0.46\n",
      "Game Episode :156/1000, High Score:25,Exploration Rate:0.46\n"
     ]
    }
   ],
   "source": [
    "\n",
    "agent = Agent(state_size, action_size) # initialise agent\n",
    "done = False\n",
    "for e in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state,[1,state_size])\n",
    "    \n",
    "    for time in range(5000):\n",
    "        env.render()\n",
    "        action = agent.act(state) #action is 0 or 1\n",
    "        next_state,reward,done,other_info = env.step(action) \n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state,[1,state_size])\n",
    "        agent.remember(state,action,reward,next_state,done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(\"Game Episode :{}/{}, High Score:{},Exploration Rate:{:.2}\".format(e,n_episodes,time,agent.epsilon))\n",
    "            break\n",
    "            \n",
    "    if len(agent.memory)>batch_size:\n",
    "        agent.train(batch_size)\n",
    "    \n",
    "    if e%50==0:\n",
    "        agent.save(output_dir+\"weights_\"+'{:04d}'.format(e)+\".hdf5\")\n",
    "        \n",
    "env.close()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
